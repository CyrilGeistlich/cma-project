---
title: "classification"
author: "Cyril Geistlich & Micha Franz"
format: html
editor: source
---
```{r}
library(ROSE)
```


# 4. Cluster
```{r}
# working_dataset_copy <- working_dataset
working_dataset <- working_dataset_copy

working_dataset <- na.omit(working_dataset)

working_dataset <- working_dataset[,-c(1,3:5)]

# Drop geom
working_dataset <- st_drop_geometry(working_dataset)

# Set threshold for parameters
working_dataset <- working_dataset[working_dataset$velocity < 250,]
working_dataset <- working_dataset[working_dataset$sinuosity < 5,]
working_dataset <- working_dataset[working_dataset$acceleration < 20,]
working_dataset <- working_dataset[working_dataset$diff_s < 1000,]

# Filter unwanted categories
working_dataset <- working_dataset[working_dataset$transport_mode != "", ]
working_dataset <- working_dataset[working_dataset$transport_mode != "Other1", ]
# working_dataset <- working_dataset[working_dataset$transport_mode != "Funicular", ]
# working_dataset <- working_dataset[working_dataset$transport_mode != "E_Kick_Scooter", ]
# working_dataset <- working_dataset[working_dataset$transport_mode != "Run", ]

# working_dataset <- working_dataset[,-c(1)]


# Show class distribution
table(working_dataset$transport_mode)
```

```{r}
# # Under sampling
# # -> Oversampling??
# 
# # Set the maximum number of entries per class
# max_entries <- 150
# 
# # Perform undersampling
# working_dataset <- working_dataset |>
#   group_by(transport_mode) |>
#   sample_n(min(n(), max_entries)) |>
#   ungroup()
# 
# # Check the resulting undersampled DataFrame
# table(working_dataset$transport_mode)
# 
# # Reset row names
# # rownames(working_dataset) <- 1:nrow(working_dataset) 
# # working_dataset <- working_dataset |>
# #   mutate(id = row_number()) # Reset IDs


```

# 5. support vector machine

```{r}


working_dataset$transport_mode <- as.factor(working_dataset$transport_mode)

TrainingIndex <- createDataPartition(working_dataset$transport_mode, p = 0.8, list = F)
TrainingSet <- working_dataset[TrainingIndex,]
TestingSet <- working_dataset[-TrainingIndex,]

# Build Training Model
model <- train(transport_mode ~ ., 
               data = TrainingSet,
               method = "svmPoly",
               na.action = na.omit,
               preprocess = c("sclae","center"),
               trControl = trainControl(method = "none"),
               tuneGrid = data.frame(degree = 1, scale = 1, C = 1)
               )

# Build CV Model
model.cv <- train(transport_mode ~ ., 
               data = TrainingSet,
               method = "svmPoly",
               na.action = na.omit,
               preprocess = c("sclae","center"),
               trControl = trainControl(method = "cv", number = 10),
               tuneGrid = data.frame(degree = 1, scale = 1, C = 1)
               )


# Make Predictions
model.training <- predict(model, TrainingSet)
model.testing <- predict(model, TestingSet)
model.cv <- predict(model.cv, TrainingSet)





# Model Performance
(model.training.confusion <- confusionMatrix(model.training, as.factor(TrainingSet$transport_mode)))
(model.testing.confusion <- confusionMatrix(model.testing, as.factor(TestingSet$transport_mode)))
(model.cv.confusion <- confusionMatrix(model.cv, as.factor(TrainingSet$transport_mode)))



```

```{r}
# Run Model on full dataset
model.final <- predict(model, working_dataset)
model.final.confusion <- confusionMatrix(model.final, as.factor(working_dataset$transport_mode))

# Save and Print final model confusion matrix
(model.final.table <- model.final.confusion[[2]])

working_dataset_result <- data.frame(working_dataset, model.final)




```
```{r Accuracy}

# Confusion Matrix
(conf_matrix <- confusionMatrix(working_dataset_result$transport_mode, working_dataset_result$model.final))

# Precision for each class
# Precision: Precision measures the proportion of correctly predicted positive instances out of the total 
# instances predicted as positive. 
# It is useful when the focus is on minimizing false positives.
(precision <- conf_matrix$byClass[, "Precision"])

# Recall for each class
# Recall (Sensitivity or True Positive Rate): Recall measures the proportion of 
# correctly predicted positive instances out of the total actual positive instances. 
# It is useful when the goal is to minimize false negatives.
(recall <- conf_matrix$byClass[, "Recall"])

# F1-Score for each class
# F1-Score: The F1-score combines precision and recall into a single metric. 
# It provides a balance between precision and recall and is useful when both false positives and false negatives are important.
(f1_score <- conf_matrix$byClass[, "F1"])
```


```{r CNN}
set.seed(100)

# BUild CNN Model
model.cnn <- train(transport_mode ~ ., 
               data = TrainingSet,
               method = "avNNet",
               na.action = na.omit,
               preprocess = c("scale", "center"),
               trControl = trainControl(method = "cv", number = 10),
               tuneGrid = data.frame(size = 1, decay = 0.1, bag = 100),
               tuneLength = 10,  # Increase the tuneLength to explore more architectures
               pool = list(pool = c(2, 2)),
               convolve = list(kernel = c(3, 3), kernel = c(5, 5))  # Change filter sizes
               )

model.cnn.cv <- train(transport_mode ~ ., 
               data = TrainingSet,
               method = "avNNet",
               na.action = na.omit,
               preprocess = c("scale", "center"),
               trControl = trainControl(method = "cv", number = 10),
               tuneGrid = data.frame(size = 1, decay = 0.1, bag = 100),
               tuneLength = 10,  # Increase the tuneLength to explore more architectures
               pool = list(pool = c(2, 2)),
               convolve = list(kernel = c(3, 3), kernel = c(5, 5))  # Change filter sizes
               )

# Make Predictions
model.cnn.training <- predict(model.cnn, TrainingSet)
model.cnn.testing <- predict(model.cnn, TestingSet)
model.cnn.cv <- predict(model.cnn.cv, TrainingSet)


(model.cnn.training.confusion <- confusionMatrix(model.cnn.training, as.factor(TrainingSet$transport_mode)))
(model.cnn.testing.confusion <- confusionMatrix(model.cnn.testing, as.factor(TestingSet$transport_mode)))
(model.cnn.cv.confusion <- confusionMatrix(model.cnn.cv, as.factor(TrainingSet$transport_mode)))

```

```{r}
# Create a contingency table of cluster and transport_mode
confusion_df <- kmeans_cluster %>%
  count(cluster, transport_mode) %>%
  spread(transport_mode, n, fill = 0)

# Remove the cluster column
confusion_df <- confusion_df[-1]

# Convert the dataframe to a matrix
confusion_matrix <- as.matrix(confusion_df)

# Add labels to the confusion matrix
colnames(confusion_matrix) <- paste("Predicted:", colnames(confusion_matrix))
rownames(confusion_matrix) <- paste("Observed:", rownames(confusion_df))

# Display the confusion matrix
confusion_matrix



```



```{r}
# Calculate the most common cluster for each label
kmeans_labels <- kmeans_cluster %>%
  group_by(transport_mode) %>%
  summarise(most_common_cluster = mode(cluster)) %>%
  ungroup()

kmeans_labels <- kmeans_cluster %>%
  count(transport_mode, cluster) %>%
  group_by(cluster)

# # Print the cluster-label assig
print(kmeans_labels)




```

```{r}
kmeans_cluster  <- kmeans_cluster |>
  group_by(cluster) 

kmeans_cluster  <- kmeans_cluster |>
  group_by(cluster, transport_mode) |>
  tally()


kmeans_cluster$labelClust <- NA
kmeans_cluster$cluster <- kmeans_cluster$cluster[NA] <- 4

for (i in 1:num_clusters){
  cluster_temp <- kmeans_cluster |>
    filter(cluster == i)
  (max <- max(cluster_temp$n))
  cluster_temp <- cluster_temp |>
    filter(cluster_temp$n == max)
  label <- cluster_temp$transport_mode
  print(label)
  kmeans_cluster$labelClust[cluster_temp$transport_mode == i] <- label
}
```

```{r}

input_plot <- ggplot(cluster_result,aes(transport_mode)) + 
  geom_bar(color = "gray") +
  labs(x = "Output Variable", y = "Frequency", title = "Histogram of Input Variables")

# Print the cluster assignments
output_plot <- ggplot(cluster_result) + 
  geom_bar(aes(clustering.cluster),color = "gray") +
  labs(x = "Output Variable", y = "Frequency", title = "Histogram of Cluster Result")

ggsave("plots/input_plot.png", plot = input_plot, width = 8, height = 6, dpi = 300)
ggsave("plots/output_plot.png", plot = output_plot, width = 8, height = 6, dpi = 300)
output_plot
input_plot
```

```{r confusion matrix}
#kmeans_cluster <- na.omit(kmeans_cluster)
(confusion_matrix <- table(kmeans_cluster$transport_mode, kmeans_cluster$labelCLust, dnn=c("Predicted, Observed")))
```

```{r Overall Accuracy}
sum(diag(confusion_matrix))/sum(confusion_matrix)
```



# 6. Evaluation

# Visualisation

```{r vis}
visual <- data.frame(st_coordinates(working_dataset), working_dataset$velocity, working_dataset$acceleration, working_dataset$sinuosity)

visual|> ggplot() +
  geom_point(aes(X,Y,color = working_dataset.velocity))

visual|> ggplot() +
  geom_point(aes(X,Y,color = working_dataset.acceleration))

visual|> ggplot() +
  geom_point(aes(X,Y,color = working_dataset.sinuosity))

ggplot(working_dataset, aes(id,sinuosity)) +
  geom_point()

cluster_result <- mutate(cluster_result, id = row_number())
working_dataset <- mutate(working_dataset, id = row_number())

cluster_join <- left_join(cluster_result, working_dataset, by = "id" )

cluster_join$correct <- cluster_join$clustering.cluster == cluster_join$transport_mode.x
cluster_join <- data.frame(cluster_join,st_coordinates(working_dataset))
map_plot <- ggplot(cluster_join) +
  geom_point(aes(X, Y, colour = correct), size = 1, alpha = 0.2)

truefalse <- ggplot(cluster_join)+
  geom_bar(aes(correct, fill = correct))

ggsave("plots/truefalse.png", plot = truefalse, width = 8, height = 6, dpi = 300)
ggsave("plots/map_plot.png", plot = map_plot, width = 8, height = 6, dpi = 300)
  
```