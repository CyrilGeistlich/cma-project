---
title: "classification"
author: "Cyril Geistlich & Micha Franz"
format: html
editor: source
---

```{r}
library(ROSE)
library(gridExtra)
```

```{r load data}
working_dataset <- read.delim("data/full_working_dataset.csv",sep=",", header = T) 
```

# Parameter Visualization

## Point Parameters

```{r}

boxplot_diff_s <- ggplot(working_dataset, aes(x = transport_mode, y = diff_s)) +
  geom_boxplot() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  ylab("sample interval [s]")

# Set threshold for parameters
working_dataset <- working_dataset[working_dataset$diff_s < 60,]

boxplot_diff_s_after <- ggplot(working_dataset, aes(x = transport_mode, y = diff_s)) +
  geom_boxplot() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  ylab("sample interval [s]")

# Display the plots side by side
grid.arrange(boxplot_diff_s, boxplot_diff_s_after, nrow = 1)
```

We remove all samples with a sampling interval larger than 5 minutes (600 seconds). We believe that at such large sampling intervals the calculated movement parameters become highly inaccurate and unrepresentative of the transport mode. This decision is based on assumptions and was not analysed in depth. We used the given tracking infrastructure by the posmo app set to a sampling interval of 10 seconds. We found the sampling interval to be highly inconsistent, with many samples having larger intervals and some being as short as 5 seconds. Fixing this issue or developing a method to handle this issue is beyond the scope of this project.

```{r visualise parameters}

boxplot_velocity <- ggplot(working_dataset, aes(x = transport_mode, y = velocity)) +
  geom_boxplot() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  ylab("velocity [m/s]")

# Set threshold for parameters
working_dataset <- working_dataset[working_dataset$velocity < 55.55,]

boxplot_velocity_after <- ggplot(working_dataset, aes(x = transport_mode, y = velocity)) +
  geom_boxplot() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  ylab("velocity [m/s]")

# Display the plots side by side
grid.arrange(boxplot_velocity, boxplot_velocity_after, nrow = 1)

```

From the velocity box plot we see that most of the issues from velocity calculation occur when travelling by train. We set the threshold for maximum velocity to 200km/h = (55.55 m/s), as no transport mode in our analysis can exceed this speed. Further we can detect many outliers in all classes. We have calculated walking speeds of up to 120km/h in our GPS data.

There are multiple reasons for such dramatic outliers in the calculated velocity. (1) Wrong Classification, even though the data is verified. (2) GPS inaccuracies, where the GPS point location is very inaccurate and therefore might mimic very fast movement.

```{r}
boxplot_acceleration <- ggplot(working_dataset, aes(x = transport_mode, y = acceleration)) +
  geom_boxplot() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  ylab("acceleration [m/s^2]")

# Set threshold for parameters
working_dataset <- working_dataset[working_dataset$acceleration < 10,]

boxplot_acceleration_after <- ggplot(working_dataset, aes(x = transport_mode, y = acceleration)) +
  geom_boxplot() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  ylab("acceleration [m/s^2]")

# Display the plots side by side
grid.arrange(boxplot_acceleration, boxplot_acceleration_after, nrow = 1)
```

The acceleration calculation is based of the time interval between samples and the calculated velocity, therefore uncertainty from these calculations is directly translated into the acceleration calculation. This is confirmed by the correlation analysis. We found strong correlation between velocity and acceleration.

## Moving Window Parameters

```{r}

boxplot_diff_s_mean <- ggplot(working_dataset, aes(x = transport_mode, y = diff_s_mean)) +
  geom_boxplot() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  ylab("sample intervall [s]")

# Set threshold for parameters
working_dataset <- working_dataset[working_dataset$diff_s_mean < 60,]

boxplot_diff_s_mean_after <- ggplot(working_dataset, aes(x = transport_mode, y = diff_s_mean)) +
  geom_boxplot() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  ylab("sample intervall [s]")

# Display the plots side by side
grid.arrange(boxplot_diff_s_mean, boxplot_diff_s_mean_after, nrow = 1)

```

Even after removing all sampling intervals we see larger differences in the moving window sampling intervals. Moving Window sampling intervals larger than 60 seconds are removed.

```{r}

boxplot_velocity_mean <- ggplot(working_dataset, aes(x = transport_mode, y = velocity_mean)) +
  geom_boxplot() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  ylab("moving window velocity[m/s]")

# Set threshold for parameters
working_dataset <- working_dataset[working_dataset$velocity_mean < 55.55,]

boxplot_velocity_mean_after <- ggplot(working_dataset, aes(x = transport_mode, y = velocity_mean)) +
  geom_boxplot() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  ylab("moving window velocity [m/s]")

# Display the plots side by side
grid.arrange(boxplot_velocity_mean, boxplot_velocity_mean_after, nrow = 1)

```
The moving window velocity gives us more accurate results. We further can reduce the number of outliers by removing such points. WE can also start to identify which classes share similar velocity values and might be hard to distinguish with the ml approach.

```{r}

boxplot_acceleration_mean <- ggplot(working_dataset, aes(x = transport_mode, y = acceleration_mean)) +
  geom_boxplot() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  ylab("moving window acceleration [m/s^2]")

# Set threshold for parameters
working_dataset <- working_dataset[working_dataset$acceleration_mean < 10,]

boxplot_acceleration_mean_after <- ggplot(working_dataset, aes(x = transport_mode, y = acceleration_mean)) +
  geom_boxplot() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  ylab("moving window acceleration [m/s^2]")

# Display the plots side by side
grid.arrange(boxplot_acceleration_mean, boxplot_acceleration_mean_after, nrow = 1)

```



# 4. Cluster

```{r}
# Code to sort by land cover -> boats etc. 
```


```{r}
working_dataset <- na.omit(working_dataset)
working_dataset <- working_dataset[,-c(1,3:5)]

# Drop geom
working_dataset <- st_drop_geometry(working_dataset)

working_dataset <- working_dataset[working_dataset$diff_s < 1000,]

# Filter unwanted categories, such which score low accuracy mainly

# Must filter:
working_dataset <- working_dataset[working_dataset$transport_mode != "", ]
working_dataset <- working_dataset[working_dataset$transport_mode != "Other1", ]

# Reclassify data with no tag or tag not of interest in the classes to other. 
#           ------------- Code here -----------------

# Remove unwanted classes
# working_dataset <- working_dataset[working_dataset$transport_mode != "Funicular", ]
# working_dataset <- working_dataset[working_dataset$transport_mode != "E_Kick_Scooter", ]
# working_dataset <- working_dataset[working_dataset$transport_mode != "Run", ]
# working_dataset <- working_dataset[working_dataset$transport_mode != "Boat", ]
# working_dataset <- working_dataset[working_dataset$transport_mode != "Skateboard", ]
# working_dataset <- working_dataset[,-c(1)]


# Show class distribution
table(working_dataset$transport_mode)
```

```{r}
# Makes model worse at this point
# # Under sampling
# # -> Oversampling??
# 
# # Set the maximum number of entries per class
# max_entries <- 1000
# 
# # Perform undersampling
# working_dataset <- working_dataset |>
#   group_by(transport_mode) |>
#   sample_n(min(n(), max_entries)) |>
#   ungroup()
# 
# # Check the resulting undersampled DataFrame
# table(working_dataset$transport_mode)
# 
# # Reset row names
# # rownames(working_dataset) <- 1:nrow(working_dataset)
# # working_dataset <- working_dataset |>
# #   mutate(id = row_number()) # Reset IDs


```

# 5. support vector machine

```{r}
set.seed(100)

working_dataset$transport_mode <- as.factor(working_dataset$transport_mode)

TrainingIndex <- createDataPartition(working_dataset$transport_mode, p = 0.8, list = F)
TrainingSet <- working_dataset[TrainingIndex,]
TestingSet <- working_dataset[-TrainingIndex,]

# Build Training Model
model <- train(transport_mode ~ ., 
               data = TrainingSet,
               method = "svmPoly",
               na.action = na.omit,
               preprocess = c("sclae","center"),
               trControl = trainControl(method = "none"),
               tuneGrid = data.frame(degree = 1, scale = 1, C = 1)
               )

# Build CV Model
model.cv <- train(transport_mode ~ ., 
               data = TrainingSet,
               method = "svmPoly",
               na.action = na.omit,
               preprocess = c("sclae","center"),
               trControl = trainControl(method = "cv", number = 10),
               tuneGrid = data.frame(degree = 1, scale = 1, C = 1)
               )

# Make Predictions
model.training <- predict(model, TrainingSet)
model.testing <- predict(model, TestingSet)
model.cv <- predict(model.cv, TrainingSet)

# Model Performance
(model.training.confusion <- confusionMatrix(model.training, as.factor(TrainingSet$transport_mode)))
(model.testing.confusion <- confusionMatrix(model.testing, as.factor(TestingSet$transport_mode)))
(model.cv.confusion <- confusionMatrix(model.cv, as.factor(TrainingSet$transport_mode)))


```

```{r}
# Run Model on full dataset
model.final <- predict(model, working_dataset)
model.final.confusion <- confusionMatrix(model.final, as.factor(working_dataset$transport_mode))

# Save and Print final model confusion matrix
(model.final.table <- model.final.confusion[[2]])

working_dataset_result <- data.frame(working_dataset, model.final)
working_dataset_result <- left_join(working_dataset_result, working_dataset_copy, 
                                    by = c("transport_mode","diff_s","diff_s_mean","angle","sinuosity","steplength","step_mean","velocity", "acceleration","acceleration_mean","velocity_mean"))

```

```{r}
working_dataset_result <- working_dataset_result %>%
    st_as_sf(coords = c("longitude", "latitude"), crs = 2056) 

map <- mapview(working_dataset_result, 
               zcol = "transport_mode", 
               layer.name = "Transport Mode",
               alpha.regions = 0.6,
               size = 3) +
  mapview(working_dataset_result, zcol = "model.final")

# Show the interactive map
map
```

```{r Accuracy}

# Confusion Matrix
(conf_matrix <- confusionMatrix(as.factor(working_dataset_result$transport_mode), working_dataset_result$model.final))

# Precision for each class
(precision <- conf_matrix$byClass[, "Precision"])

# Recall for each class
(recall <- conf_matrix$byClass[, "Recall"])

# F1-Score for each class
(f1_score <- conf_matrix$byClass[, "F1"])
```

Confusion Matrix: 

Precision: Precision measures the proportion of correctly predicted positive instances out of the total 
instances predicted as positive. 
It is useful when the focus is on minimizing false positives.

Recall (Sensitivity or True Positive Rate): Recall measures the proportion of
correctly predicted positive instances out of the total actual positive instances.
It is useful when the goal is to minimize false negatives.

F1-Score: The F1-score combines precision and recall into a single metric.
It provides a balance between precision and recall and is useful when both false positives and false negatives are important.

```{r}
# Confusion Matrix for new results
(conf_matrix_2 <- confusionMatrix(as.factor(working_dataset_result$transport_mode), working_dataset_result$model.final))

# Precision for each class
(precision_2 <- conf_matrix_2$byClass[, "Precision"])

# Recall for each class
(recall_2 <- conf_matrix_2$byClass[, "Recall"])

# F1-Score for each class
(f1_score_2 <- conf_matrix_2$byClass[, "F1"])
```

# K-Means

```{r}


model.kmeans <- train(transport_mode ~ ., 
               data = TrainingSet,
               method = "kmeans",
               na.action = na.omit,
               preprocess = c("scale", "center"),
               trControl = trainControl(method = "none"),
               tuneGrid = data.frame(degree = 1, scale = 1, C = 1)
               )

# Build CV Model
model.kmeans.cv <- train(transport_mode ~ ., 
               data = TrainingSet,
               method = "kmeans",
               na.action = na.omit,
               preprocess = c("sclae","center"),
               trControl = trainControl(method = "cv", number = 10),
               tuneGrid = data.frame(degree = 1, scale = 1, C = 1)
               )


# Make Predictions
model.kmeans.training <- predict(model.kmeans, TrainingSet)
model.kmeans.testing <- predict(model.kmeans, TestingSet)
model.kmeans.cv <- predict(model.kmeans.cv, TrainingSet)





# Model Performance
(model.kmeans.training.confusion <- confusionMatrix(model.kmeans.training, as.factor(TrainingSet$transport_mode)))
(model.kmeans.testing.confusion <- confusionMatrix(model.kmeans.testing, as.factor(TestingSet$transport_mode)))
(model.kmeans.cv.confusion <- confusionMatrix(model.kmeans.cv, as.factor(TrainingSet$transport_mode)))


```

# Neural Net

```{r CNN}
set.seed(100)

# BUild CNN Model
model.cnn <- train(transport_mode ~ ., 
               data = TrainingSet,
               method = "avNNet",
               na.action = na.omit,
               preprocess = c("scale", "center"),
               trControl = trainControl(method = "cv", number = 10),
               tuneGrid = data.frame(size = 1, decay = 0.1, bag = 100),
               tuneLength = 10,  # Increase the tuneLength to explore more architectures
               pool = list(pool = c(2, 2)),
               convolve = list(kernel = c(3, 3), kernel = c(5, 5))  # Change filter sizes
               )

model.cnn.cv <- train(transport_mode ~ ., 
               data = TrainingSet,
               method = "avNNet",
               na.action = na.omit,
               preprocess = c("scale", "center"),
               trControl = trainControl(method = "cv", number = 10),
               tuneGrid = data.frame(size = 1, decay = 0.1, bag = 100),
               tuneLength = 10,  # Increase the tuneLength to explore more architectures
               pool = list(pool = c(2, 2)),
               convolve = list(kernel = c(3, 3), kernel = c(5, 5))  # Change filter sizes
               )

# Make Predictions
model.cnn.training <- predict(model.cnn, TrainingSet)
model.cnn.testing <- predict(model.cnn, TestingSet)
model.cnn.cv <- predict(model.cnn.cv, TrainingSet)


(model.cnn.training.confusion <- confusionMatrix(model.cnn.training, as.factor(TrainingSet$transport_mode)))
(model.cnn.testing.confusion <- confusionMatrix(model.cnn.testing, as.factor(TestingSet$transport_mode)))
(model.cnn.cv.confusion <- confusionMatrix(model.cnn.cv, as.factor(TrainingSet$transport_mode)))

```


# Post Processing

```{r}
# Run a loop to identify outlier points in classification. If prevous and following x points are identical, 
# but the middle one is different it is changed

# Assuming your dataframe is named "df" and contains the attribute "model.final"

# Define the number of previous and following points to consider
x <- 3
count <- 0


# Loop through the dataframe and update the value of "model.final" if the surrounding points match
for (i in 1:nrow(working_dataset_result)) {
  current_value <- working_dataset_result$model.final[i]
  for (j in 1:x) {
      if (j >= x) {
        previous_values <- working_dataset_result$model.final[(i - x):(i - 1)]
        if (all(previous_values == current_value)) {
            working_dataset_result$model.final[(i - x + 1):i] <- current_value
    }
  }
  # Check previous x points

  }
       # Check following x points, does not work :(

       if (i <= (nrow(working_dataset_result) - x + 1)) {
         following_values <- working_dataset_result$model.final[(i + 1):(i + x)]
         if (all(following_values == current_value)) {
          working_dataset_result$model.final[i:(i + x - 1)] <- current_value
        }
      }
}

print(count)

```


# 6. Evaluation

# Visualisation

