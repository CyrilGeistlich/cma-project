---
title: "classification"
author: "Cyril Geistlich & Micha Franz"
format: html
editor: source
---

```{r}
library(ROSE)
library(gridExtra)
```

```{r load data}
working_dataset <- read.delim("data/full_working_dataset.csv",sep=",", header = T) 

```

# Parameter Visualization

## Point Parameters

```{r}

boxplot_diff_s <- ggplot(working_dataset, aes(x = transport_mode, y = diff_s)) +
  geom_boxplot() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  ylab("sample interval [s]")

# Set threshold for parameters
working_dataset <- working_dataset[working_dataset$diff_s < 60,]

boxplot_diff_s_after <- ggplot(working_dataset, aes(x = transport_mode, y = diff_s)) +
  geom_boxplot() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  ylab("sample interval [s]")

# Display the plots side by side
grid.arrange(boxplot_diff_s, boxplot_diff_s_after, nrow = 1)
```

We remove all samples with a sampling interval larger than 5 minutes (600 seconds). We believe that at such large sampling intervals the calculated movement parameters become highly inaccurate and unrepresentative of the transport mode. This decision is based on assumptions and was not analysed in depth. We used the given tracking infrastructure by the posmo app set to a sampling interval of 10 seconds. We found the sampling interval to be highly inconsistent, with many samples having larger intervals and some being as short as 5 seconds. Fixing this issue or developing a method to handle this issue is beyond the scope of this project.

```{r visualise parameters}

boxplot_velocity <- ggplot(working_dataset, aes(x = transport_mode, y = velocity)) +
  geom_boxplot() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  ylab("velocity [m/s]")

# Set threshold for parameters
working_dataset <- working_dataset[working_dataset$velocity < 55.55,]

boxplot_velocity_after <- ggplot(working_dataset, aes(x = transport_mode, y = velocity)) +
  geom_boxplot() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  ylab("velocity [m/s]")

# Display the plots side by side
grid.arrange(boxplot_velocity, boxplot_velocity_after, nrow = 1)

```

From the velocity box plot we see that most of the issues from velocity calculation occur when travelling by train. We set the threshold for maximum velocity to 200km/h = (55.55 m/s), as no transport mode in our analysis can exceed this speed. Further we can detect many outliers in all classes. We have calculated walking speeds of up to 120km/h in our GPS data.

There are multiple reasons for such dramatic outliers in the calculated velocity. (1) Wrong Classification, even though the data is verified. (2) GPS inaccuracies, where the GPS point location is very inaccurate and therefore might mimic very fast movement.

```{r}
boxplot_acceleration <- ggplot(working_dataset, aes(x = transport_mode, y = acceleration)) +
  geom_boxplot() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  ylab("acceleration [m/s^2]")

# Set threshold for parameters
working_dataset <- working_dataset[working_dataset$acceleration < 10,]

boxplot_acceleration_after <- ggplot(working_dataset, aes(x = transport_mode, y = acceleration)) +
  geom_boxplot() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  ylab("acceleration [m/s^2]")

# Display the plots side by side
grid.arrange(boxplot_acceleration, boxplot_acceleration_after, nrow = 1)
```

The acceleration calculation is based of the time interval between samples and the calculated velocity, therefore uncertainty from these calculations is directly translated into the acceleration calculation. This is confirmed by the correlation analysis. We found strong correlation between velocity and acceleration.

## Moving Window Parameters

```{r}

boxplot_diff_s_mean <- ggplot(working_dataset, aes(x = transport_mode, y = diff_s_mean)) +
  geom_boxplot() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  ylab("sample intervall [s]")

# Set threshold for parameters
working_dataset <- working_dataset[working_dataset$diff_s_mean < 60,]

boxplot_diff_s_mean_after <- ggplot(working_dataset, aes(x = transport_mode, y = diff_s_mean)) +
  geom_boxplot() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  ylab("sample intervall [s]")

# Display the plots side by side
grid.arrange(boxplot_diff_s_mean, boxplot_diff_s_mean_after, nrow = 1)

```

Even after removing all sampling intervals we see larger differences in the moving window sampling intervals. Moving Window sampling intervals larger than 60 seconds are removed.

```{r}

boxplot_velocity_mean <- ggplot(working_dataset, aes(x = transport_mode, y = velocity_mean)) +
  geom_boxplot() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  ylab("moving window velocity[m/s]")

# Set threshold for parameters
working_dataset <- working_dataset[working_dataset$velocity_mean < 55.55,]

boxplot_velocity_mean_after <- ggplot(working_dataset, aes(x = transport_mode, y = velocity_mean)) +
  geom_boxplot() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  ylab("moving window velocity [m/s]")

# Display the plots side by side
grid.arrange(boxplot_velocity_mean, boxplot_velocity_mean_after, nrow = 1)

```
The moving window velocity gives us more accurate results. We further can reduce the number of outliers by removing such points. WE can also start to identify which classes share similar velocity values and might be hard to distinguish with the ml approach.

```{r}

boxplot_acceleration_mean <- ggplot(working_dataset, aes(x = transport_mode, y = acceleration_mean)) +
  geom_boxplot() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  ylab("moving window acceleration [m/s^2]")

# Set threshold for parameters
working_dataset <- working_dataset[working_dataset$acceleration_mean < 10,]

boxplot_acceleration_mean_after <- ggplot(working_dataset, aes(x = transport_mode, y = acceleration_mean)) +
  geom_boxplot() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  ylab("moving window acceleration [m/s^2]")

# Display the plots side by side
grid.arrange(boxplot_acceleration_mean, boxplot_acceleration_mean_after, nrow = 1)

```

# 4. Cluster

```{r}
# Code to sort by land cover -> boats etc. 
```


```{r}
working_dataset <- na.omit(working_dataset)


# Filter unwanted categories, such which score low accuracy mainly

# Remove unwanted classes
working_dataset <- working_dataset[working_dataset$transport_mode != "", ]
working_dataset <- working_dataset[working_dataset$transport_mode != "Other1", ]
# working_dataset <- working_dataset[working_dataset$transport_mode != "Funicular", ]
# working_dataset <- working_dataset[working_dataset$transport_mode != "E_Kick_Scooter", ]
# working_dataset <- working_dataset[working_dataset$transport_mode != "Run", ]
# working_dataset <- working_dataset[working_dataset$transport_mode != "Boat", ]
# working_dataset <- working_dataset[working_dataset$transport_mode != "Skateboard", ]

# Move less relevant modes into category "other"
working_dataset$transport_mode[working_dataset$transport_mode == "Funicular"] <- "Other"
working_dataset$transport_mode[working_dataset$transport_mode == "E_Kick_Scooter"] <- "Other"
working_dataset$transport_mode[working_dataset$transport_mode == "Run"] <- "Other"
working_dataset$transport_mode[working_dataset$transport_mode == "Boat"] <- "Other"
working_dataset$transport_mode[working_dataset$transport_mode == "Skateboard"] <- "Other"

# Show class distribution
ggplot(working_dataset) + 
  geom_bar(aes(x = transport_mode)) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Create Copy of df for getting geometry back in df
working_dataset_copy <- working_dataset

#Drop unwanted/Geom Columns
working_dataset <- working_dataset[,-c(1,3:5)]
working_dataset <- st_drop_geometry(working_dataset)


```

```{r}
# Makes model worse at this point
# # Under sampling
# # -> Oversampling??
# 
# # Set the maximum number of entries per class
# max_entries <- 1000
# 
# # Perform undersampling
# working_dataset <- working_dataset |>
#   group_by(transport_mode) |>
#   sample_n(min(n(), max_entries)) |>
#   ungroup()
# 
# # Check the resulting undersampled DataFrame
# table(working_dataset$transport_mode)
# 
# # Reset row names
# # rownames(working_dataset) <- 1:nrow(working_dataset)
# # working_dataset <- working_dataset |>
# #   mutate(id = row_number()) # Reset IDs


```

# 5. support vector machine (SVM)

```{r}
# Define Control 
fitControl <- trainControl(## 10-fold CV
                           method = "repeatedcv",
                           number = 10,
                           ## repeated ten times
                           repeats = 3)
```
 

# 5.1 Liner SVM

```{r}

# Perform Linear SVM
model.svmL <- train(transport_mode ~ ., 
               data = TrainingSet,
               method = "svmLinear",
               na.action = na.omit,
               preprocess = c("scale", "center"),
               trControl = trainControl(method = "none"),
               tuneGrid = data.frame(degree = 1, scale = 1, C = 1)
               )

# Perform Linear SVM with 10-fold Cross Validation
model.svmL.cv <- train(transport_mode ~ ., 
               data = TrainingSet,
               method = "svmLinear",
               na.action = na.omit,
               preprocess = c("sclae","center"),
               trControl = trainControl(method = "cv", number = 10),
               TuneGrid = expand.grid(C = seq(0, 2, length = 20) # Find best Fit Model
               ))

# Show Best Tune
(model.svmL.cv$bestTune)

# Make Predictions
model.svmL.training <- predict(model.svmL, TrainingSet)
model.svmL.testing <- predict(model.svmL, TestingSet)
model.svmL.cv.training <- predict(model.svmL.cv, TrainingSet)
model.svmL.cv.testing <- predict(model.svmL.cv, TrainingSet)

# Model Performance
(model.svmL.training.confusion <- confusionMatrix(model.svmL.training, as.factor(TrainingSet$transport_mode)))
(model.svmL.testing.confusion <- confusionMatrix(model.svmL.testing, as.factor(TestingSet$transport_mode)))
(model.svmL.cv.training.confusion <- confusionMatrix(model.svmL.cv.training, as.factor(TrainingSet$transport_mode)))
(model.svmL.cv.testing.confusion <- confusionMatrix(model.svmL.cv.testing, as.factor(TrainingSet$transport_mode)))

```

```{r}
set.seed(100)

working_dataset$transport_mode <- as.factor(working_dataset$transport_mode)

TrainingIndex <- createDataPartition(working_dataset$transport_mode, p = 0.8, list = F)
TrainingSet <- working_dataset[TrainingIndex,]
TestingSet <- working_dataset[-TrainingIndex,]



# Build Training Model
model.svm <- train(transport_mode ~ ., 
               data = TrainingSet,
               method = "svmPoly",
               na.action = na.omit,
               preprocess = c("sclae","center"),
               trControl = trainControl(method = "none"),
               tuneGrid = data.frame(degree = 1, scale = 1, C = 1))
               

# Build CV Model (long processing)
TrainingSet$transport_mode <- as.character(TrainingSet$transport_mode)
model.svm.cv <- train(transport_mode ~ ., 
               data = TrainingSet,
               method = "svmPoly",
               na.action = na.omit,
               preprocess = c("sclae","center"),
               trControl = fitControl,
               tuneLength = 4) # Fit Model
               
(model.svm.cv$bestTune)

# Make Predictions
model.svm.training <- predict(model.svm, TrainingSet)
model.svm.testing <- predict(model.svm, TestingSet)

# Make Predictions from Cross Validation model
model.svm.cv.training <- predict(model.svm.cv, TrainingSet)
model.svm.cv.testing <- predict(model.svm.cv, TestingSet)

# Model Performance
(model.svm.training.confusion <- confusionMatrix(model.svm.training, as.factor(TrainingSet$transport_mode)))
(model.svm.testing.confusion <- confusionMatrix(model.svm.testing, as.factor(TestingSet$transport_mode)))
(model.svm.cv.confusion <- confusionMatrix(model.svm.cv.training, as.factor(TrainingSet$transport_mode)))
(model.svm.cv.testing.confusion <- confusionMatrix(model.svm.cv.testing, as.factor(TestingSet$transport_mode)))

print(model.svm)
print(model.svm.cv)
```

```{r}

set.seed(101)
# Run Model on full dataset
model.svm.final <- predict(model.svm.cv, working_dataset)
model.svm.final.confusion <- confusionMatrix(model.svm.final, as.factor(working_dataset$transport_mode))

# Save and Print final model confusion matrix
(model.final.table <- model.final.confusion[[2]])

colnames <- colnames(working_dataset)

working_dataset_result <- data.frame(working_dataset, model.final)
working_dataset_result <- left_join(working_dataset_result, working_dataset_copy, 
                                    by = c("transport_mode","diff_s","diff_s_mean","angle","sinuosity","steplength","step_mean","velocity", "acceleration","acceleration_mean","velocity_mean"))

write.csv(working_dataset_result, "data/working_dataset_result.csv", row.names = F)

```


```{r}

working_dataset_result <- read.delim("data/working_dataset_result.csv", sep=",", header = T) 

working_dataset_result <- working_dataset_result %>%
    st_as_sf(coords = c("longitude", "latitude"), crs = 4326) 

map <- mapview(working_dataset_result, 
               zcol = "transport_mode", 
               layer.name = "Transport Mode",
               alpha.regions = 0.6,
               size = 3) +
  mapview(working_dataset_result, zcol = "model.final")

# Show the interactive map
map
```

```{r Accuracy}

# Confusion Matrix
# Note: Only works if result and working dataset have identical transport modes as factors.
(conf_matrix <- confusionMatrix(as.factor(working_dataset_result$transport_mode), working_dataset_result$model.final))

# Precision for each class
(precision <- conf_matrix$byClass[, "Precision"])

# Recall for each class
(recall <- conf_matrix$byClass[, "Recall"])

# F1-Score for each class
(f1_score <- conf_matrix$byClass[, "F1"])
```

Confusion Matrix: 

Precision: Precision measures the proportion of correctly predicted positive instances out of the total 
instances predicted as positive. 
It is useful when the focus is on minimizing false positives.

Recall (Sensitivity or True Positive Rate): Recall measures the proportion of
correctly predicted positive instances out of the total actual positive instances.
It is useful when the goal is to minimize false negatives.

F1-Score: The F1-score combines precision and recall into a single metric.
It provides a balance between precision and recall and is useful when both false positives and false negatives are important.



# Space for another model

```{r}


model.kmeans <- train(transport_mode ~ ., 
               data = TrainingSet,
               method = "",
               na.action = na.omit,
               preprocess = c("scale", "center"),
               trControl = trainControl(method = "none"),
               tuneGrid = data.frame(degree = 1, scale = 1, C = 1)
               )

# Build CV Model
model.kmeans.cv <- train(transport_mode ~ ., 
               data = TrainingSet,
               method = "",
               na.action = na.omit,
               preprocess = c("sclae","center"),
               trControl = trainControl(method = "cv", number = 10),
               tuneGrid = data.frame(degree = 1, scale = 1, C = 1)
               )


# Make Predictions
model.kmeans.training <- predict(model.kmeans, TrainingSet)
model.kmeans.testing <- predict(model.kmeans, TestingSet)
model.kmeans.cv <- predict(model.kmeans.cv, TrainingSet)





# Model Performance
(model.kmeans.training.confusion <- confusionMatrix(model.kmeans.training, as.factor(TrainingSet$transport_mode)))
(model.kmeans.testing.confusion <- confusionMatrix(model.kmeans.testing, as.factor(TestingSet$transport_mode)))
(model.kmeans.cv.confusion <- confusionMatrix(model.kmeans.cv, as.factor(TrainingSet$transport_mode)))


```

# Neural Net

```{r CNN}
# set.seed(100)
# 
# # BUild CNN Model
# model.cnn <- train(transport_mode ~ ., 
#                data = TrainingSet,
#                method = "avNNet",
#                na.action = na.omit,
#                preprocess = c("scale", "center"),
#                trControl = trainControl(method = "cv", number = 10),
#                tuneGrid = data.frame(size = 1, decay = 0.1, bag = 100),
#                tuneLength = 10,  # Increase the tuneLength to explore more architectures
#                pool = list(pool = c(2, 2)),
#                convolve = list(kernel = c(3, 3), kernel = c(5, 5))  # Change filter sizes
#                )
# 
# model.cnn.cv <- train(transport_mode ~ ., 
#                data = TrainingSet,
#                method = "avNNet",
#                na.action = na.omit,
#                preprocess = c("scale", "center"),
#                trControl = trainControl(method = "cv", number = 10),
#                tuneGrid = data.frame(size = 1, decay = 0.1, bag = 100),
#                tuneLength = 10,  # Increase the tuneLength to explore more architectures
#                pool = list(pool = c(2, 2)),
#                convolve = list(kernel = c(3, 3), kernel = c(5, 5))  # Change filter sizes
#                )
# 
# # Make Predictions
# model.cnn.training <- predict(model.cnn, TrainingSet)
# model.cnn.testing <- predict(model.cnn, TestingSet)
# model.cnn.cv <- predict(model.cnn.cv, TrainingSet)
# 
# 
# (model.cnn.training.confusion <- confusionMatrix(model.cnn.training, as.factor(TrainingSet$transport_mode)))
# (model.cnn.testing.confusion <- confusionMatrix(model.cnn.testing, as.factor(TestingSet$transport_mode)))
# (model.cnn.cv.confusion <- confusionMatrix(model.cnn.cv, as.factor(TrainingSet$transport_mode)))

```


# Post Processing

```{r}
# Run a loop to identify outlier points in classification. If prevous and following x points are identical, 
# but the middle one is different it is changed

# Assuming your dataframe is named "df" and contains the attribute "model.final"

# Define the number of previous and following points to consider
x <- 3
count <- 0


# Loop through the dataframe and update the value of "model.final" if the surrounding points match
for (i in 1:nrow(working_dataset_result)) {
  current_value <- working_dataset_result$model.final[i]
  for (j in 1:x) {
    
    # Find x-Previous & -Following Values before point i
    previous_values <- working_dataset_result$model.final[(i - x):(i - 1)]
    following_values <- working_dataset_result$model.final[(i + 1):(i + x)]
        
    # Check if all previous & following values are equal and are different from current value
    if (all(previous_values == following_values && previous_values != current_value && previous_values != currentvalue)) {
            print(i)  
          #working_dataset_result$model.final[(i - x + 1):i] <- current_value
    }
  }
}


print(count)

```

```{r}
# Confusion Matrix for new results
(conf_matrix_2 <- confusionMatrix(as.factor(working_dataset_result$transport_mode), working_dataset_result$model.final))

# Precision for each class
(precision_2 <- conf_matrix_2$byClass[, "Precision"])

# Recall for each class
(recall_2 <- conf_matrix_2$byClass[, "Recall"])

# F1-Score for each class
(f1_score_2 <- conf_matrix_2$byClass[, "F1"])
```

# 6. Evaluation

# Visualisation

# Literature

https://findingspress.org/article/14520-classifying-transport-mode-from-global-positioning-systems-and-accelerometer-data-a-machine-learning-approach

