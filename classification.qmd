---
title: "classification"
author: "Cyril Geistlich & Micha Franz"
format: html
editor: source
---

```{r}
library(ROSE)
library(gridExtra)
```

```{r load data}
working_dataset <- read.delim("data/full_working_dataset.csv",sep=",", header = T) 
posmo_pool <- read.delim("data/full_posmo_pool_dataset.csv",sep=",", header = T) 

working_dataset <- rbind(working_dataset, posmo_pool)

working_dataset <- na.omit(working_dataset)
# Show class distribution
ggplot(working_dataset) + 
  geom_bar(aes(x = transport_mode)) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

```{r}
# Remove unwanted classes
working_dataset <- working_dataset[working_dataset$transport_mode != "", ]
working_dataset <- working_dataset[working_dataset$transport_mode != "Other1", ]
# working_dataset <- working_dataset[working_dataset$transport_mode != "Funicular", ]
# working_dataset <- working_dataset[working_dataset$transport_mode != "E_Kick_Scooter", ]
# working_dataset <- working_dataset[working_dataset$transport_mode != "Run", ]
# working_dataset <- working_dataset[working_dataset$transport_mode != "Boat", ]
# working_dataset <- working_dataset[working_dataset$transport_mode != "Skateboard", ]

# Move less relevant modes into category "other"
working_dataset$transport_mode[working_dataset$transport_mode == "Funicular"] <- "Other"
working_dataset$transport_mode[working_dataset$transport_mode == "E_Kick_Scooter"] <- "Other"
working_dataset$transport_mode[working_dataset$transport_mode == "Run"] <- "Other"
working_dataset$transport_mode[working_dataset$transport_mode == "Skateboard"] <- "Other"
working_dataset$transport_mode[working_dataset$transport_mode == "Airplane"] <- "Other"
working_dataset$transport_mode[working_dataset$transport_mode == "E_Bike"] <- "Other"
# working_dataset$transport_mode[working_dataset$transport_mode == "Boat"] <- "Other"

ggplot(working_dataset) + 
  geom_bar(aes(x = transport_mode)) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

table(working_dataset$transport_mode)
```

# Parameter Visualization

## Point Parameters

```{r, warning = F}

boxplot_diff_s <- ggplot(working_dataset,aes(x = transport_mode, y = diff_s)) +
  geom_boxplot() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  ylab("sample interval [s]")

plot_diff_s <- ggplot(working_dataset) + 
  geom_line(aes(x = id, y = diff_s,colour = transport_mode)) + 
  xlim(c(0,1000)) + # Subset of id 1 to 1000
  ylim(c(0,120)) +
  ylab("sample interval [s]")


# Set threshold for parameters
working_dataset <- working_dataset[working_dataset$diff_s < 60,]

boxplot_diff_s_after <- ggplot(working_dataset) +
  geom_boxplot(aes(x = working_dataset$transport_mode, y = working_dataset$diff_s)) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  ylab("sample interval [s]")

plot_diff_s_after <- ggplot(working_dataset, aes(x = id, y = diff_s,colour = transport_mode)) + 
  geom_line() + 
  xlim(c(0,1000)) + # Subset of id 1 to 1000
  ylab("sample interval [s]")

# Display the plots side by side
grid.arrange(boxplot_diff_s, boxplot_diff_s_after, nrow = 2)
grid.arrange(plot_diff_s,plot_diff_s_after, nrow = 2)

```

We remove all samples with a sampling interval larger than 5 minutes (600 seconds). We believe that at such large sampling intervals the calculated movement parameters become highly inaccurate and unrepresentative of the transport mode. This decision is based on assumptions and was not analysed in depth. We used the given tracking infrastructure by the posmo app set to a sampling interval of 10 & 15 seconds. We found the sampling interval to be highly inconsistent, with many samples having larger intervals and some being as short as 5 seconds. This is also a result of our point removal functions, which reduce angle etc.. However, If we see the line plots we find that the reduction of the sample interval down to 60 seconds gives the data a smoothing effect. Further reduction or interpolation to even out the sampling interval might improve the model results. 

If so done a solution for handling large time difference between two consecutive time samples is needed. 


```{r visualise parameters}

boxplot_velocity <- ggplot(working_dataset, aes(x = transport_mode, y = velocity)) +
  geom_boxplot() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  ylab("velocity [m/s]")

# Set threshold for parameters
working_dataset <- working_dataset[working_dataset$velocity < 55.55,]

boxplot_velocity_after <- ggplot(working_dataset, aes(x = transport_mode, y = velocity)) +
  geom_boxplot() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  ylab("velocity [m/s]")

# Display the plots side by side
grid.arrange(boxplot_velocity, boxplot_velocity_after, nrow = 1)

```

From the velocity box plot we see that most of the issues from velocity calculation occur when travelling by train. We set the threshold for maximum velocity to 200km/h = (55.55 m/s), as no transport mode in our analysis can exceed this speed. Further we can detect many outliers in all classes. We have calculated walking speeds of up to 120km/h in our GPS data.

There are multiple reasons for such dramatic outliers in the calculated velocity. (1) Wrong Classification, even though the data is verified. (2) GPS inaccuracies, where the GPS point location is very inaccurate and therefore might mimic very fast movement.

```{r}
boxplot_acceleration <- ggplot(working_dataset, aes(x = transport_mode, y = acceleration)) +
  geom_boxplot() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  ylab("acceleration [m/s^2]")

# Set threshold for parameters
working_dataset <- working_dataset[working_dataset$acceleration < 10,]

boxplot_acceleration_after <- ggplot(working_dataset, aes(x = transport_mode, y = acceleration)) +
  geom_boxplot() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  ylab("acceleration [m/s^2]")

# Display the plots side by side
grid.arrange(boxplot_acceleration, boxplot_acceleration_after, nrow = 1)
```

The acceleration calculation is based of the time interval between samples and the calculated velocity, therefore uncertainty from these calculations is directly translated into the acceleration calculation. This is confirmed by the correlation analysis. We found strong correlation between velocity and acceleration.

## Moving Window Parameters

```{r}

boxplot_diff_s_mean <- ggplot(working_dataset, aes(x = transport_mode, y = diff_s_mean)) +
  geom_boxplot() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  ylab("sample intervall [s]")

# Set threshold for parameters
working_dataset <- working_dataset[working_dataset$diff_s_mean < 60,]

boxplot_diff_s_mean_after <- ggplot(working_dataset, aes(x = transport_mode, y = diff_s_mean)) +
  geom_boxplot() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  ylab("sample intervall [s]")

# Display the plots side by side
grid.arrange(boxplot_diff_s_mean, boxplot_diff_s_mean_after, nrow = 1)

```

Even after removing all sampling intervals we see larger differences in the moving window sampling intervals. Moving Window sampling intervals larger than 60 seconds are removed.

```{r}

boxplot_velocity_mean <- ggplot(working_dataset, aes(x = transport_mode, y = velocity_mean)) +
  geom_boxplot() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  ylab("moving window velocity[m/s]")

# Set threshold for parameters
working_dataset <- working_dataset[working_dataset$velocity_mean < 55.55,]

boxplot_velocity_mean_after <- ggplot(working_dataset, aes(x = transport_mode, y = velocity_mean)) +
  geom_boxplot() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  ylab("moving window velocity [m/s]")

# Display the plots side by side
grid.arrange(boxplot_velocity_mean, boxplot_velocity_mean_after, nrow = 1)

```
The moving window velocity gives us more accurate results. We further can reduce the number of outliers by removing such points. WE can also start to identify which classes share similar velocity values and might be hard to distinguish with the ml approach.

```{r}

boxplot_acceleration_mean <- ggplot(working_dataset, aes(x = transport_mode, y = acceleration_mean)) +
  geom_boxplot() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  ylab("moving window acceleration [m/s^2]")

# Set threshold for parameters
working_dataset <- working_dataset[working_dataset$acceleration_mean < 10,]

boxplot_acceleration_mean_after <- ggplot(working_dataset, aes(x = transport_mode, y = acceleration_mean)) +
  geom_boxplot() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  ylab("moving window acceleration [m/s^2]")

# Display the plots side by side
grid.arrange(boxplot_acceleration_mean, boxplot_acceleration_mean_after, nrow = 1)

```

# 4. Cluster



We have an imbalanced data set. The train class is overrepresented, as we do not expect this to be the main mode of transport. Rather we believe that the phone is used more often in the train than in other transport vehicles, and therefore generated more consistent GPS signals. 

# Under Sampling

```{r}


# Set the maximum number of entries per class
max_entries <- 500

# Perform undersampling
working_dataset <- working_dataset |>
  group_by(transport_mode) |>
  sample_n(min(n(), max_entries)) |>
  ungroup()

# Check the resulting undersampled DataFrame
table(working_dataset$transport_mode)

working_dataset_copy <- working_dataset
```
We under sample our classes to 500 entries. This allows us to achieve balanced classes for our models. 

```{r}
#Drop unwanted/Geom Columns

working_dataset <- working_dataset[,-c(1,3:5)]
working_dataset <- st_drop_geometry(working_dataset)
```


# 5. support vector machine (SVM)

We create code for a 10 fold cross validation with 3 repeats. 

```{r}
# Define Control 
fitControl <- trainControl(## 10-fold CV
                           method = "repeatedcv",
                           number = 10,
                           ## repeated ten times
                           repeats = 3)
```

We create a training and a test data set. The training data set containts 80% of the data points and the test set contains 20% of the data points. 

```{r}
working_dataset$transport_mode <- as.factor(working_dataset$transport_mode)

TrainingIndex <- createDataPartition(working_dataset$transport_mode, p = 0.8, list = F)
TrainingSet <- working_dataset[TrainingIndex,]
TestingSet <- working_dataset[-TrainingIndex,]

```

# 5.1 Liner SVM

We test a linear support vector machine and evaluate performance. 

```{r}
# Set seed for reproduceability
set.seed(100)

# Perform Linear SVM
model.svmL <- train(transport_mode ~ ., 
               data = TrainingSet,
               method = "svmLinear",
               na.action = na.omit,
               preprocess = c("scale", "center"),
               trControl = trainControl(method = "none"),
               tuneGrid = data.frame(C = 1),
               )

# Perform Linear SVM with 10-fold Cross Validation
model.svmL.cv <- train(transport_mode ~ ., 
               data = TrainingSet,
               method = "svmLinear",
               na.action = na.omit,
               preprocess = c("sclae","center"),
               trControl = fitControl,
               tuneGrid = expand.grid(C = seq(1.5, 5, length = 3) # Find best Fit Model
               ))

# Show Best Tune
(model.svmL.cv$bestTune)

# Make Predictions
model.svmL.training <- predict(model.svmL, TrainingSet)
model.svmL.testing <- predict(model.svmL, TestingSet)
model.svmL.cv.training <- predict(model.svmL.cv, TrainingSet)
model.svmL.cv.testing <- predict(model.svmL.cv, TrainingSet)

# Model Performance
(model.svmL.training.confusion <- confusionMatrix(model.svmL.training, as.factor(TrainingSet$transport_mode)))
(model.svmL.testing.confusion <- confusionMatrix(model.svmL.testing, as.factor(TestingSet$transport_mode)))
(model.svmL.cv.training.confusion <- confusionMatrix(model.svmL.cv.training, as.factor(TrainingSet$transport_mode)))
(model.svmL.cv.testing.confusion <- confusionMatrix(model.svmL.cv.testing, as.factor(TrainingSet$transport_mode)))

# Save the models
saveRDS(model.svmL, "models/model_svmL.rds")
saveRDS(model.svmL.cv, "models/model_svmL_cv.rds")

```
## Reduced Number of Classes
If we categorize only Bike, Bus, Car Train, Tram, Walk & Others we found the best model with C = 1. 
The accuracy lies at 81.9%

## All Classes

If all Classes are considered we achieve: 



# Radial Support Vector Machine

```{r}
# Set seed for reproduceability
set.seed(100)

# Build Training Model
model.svmRadial <- train(transport_mode ~ .,
                         data = TrainingSet,
                         method = "svmRadial",
                         na.action = na.omit,
                         preprocess = c("scale", "center"),
                         trControl = trainControl(method = "none"),
                         tuneGrid = expand.grid(sigma = 0.8683492, C = 5)
)             

# Build CV Model (long processing!!!)
TrainingSet$transport_mode <- as.character(TrainingSet$transport_mode)
model.svmRadial.cv <- train(transport_mode ~ .,
                            data = TrainingSet,
                            method = "svmRadial",
                            na.action = na.omit,
                            preprocess = c("scale", "center"),
                            trControl = fitControl,
                            tuneGrid = expand.grid(sigma = 0.8683492, C = 5)
)
               
(model.svmRadial.cv$bestTune)

# Make Predictions
model.svmRadial.training <- predict(model.svmRadial, TrainingSet)
model.svmRadial.testing <- predict(model.svmRadial, TestingSet)

# Make Predictions from Cross Validation model
model.svmRadial.cv.training <- predict(model.svmRadial.cv, TrainingSet)
model.svmRadial.cv.testing <- predict(model.svmRadial.cv, TestingSet)

# Model Performance
(model.svmRadial.training.confusion <- confusionMatrix(model.svmRadial.training, as.factor(TrainingSet$transport_mode)))
(model.svmRadial.testing.confusion <- confusionMatrix(model.svmRadial.testing, as.factor(TestingSet$transport_mode)))
(model.svmRadial.cv.confusion <- confusionMatrix(model.svmRadial.cv.training, as.factor(TrainingSet$transport_mode)))
(model.svmRadial.cv.testing.confusion <- confusionMatrix(model.svmRadial.cv.testing, as.factor(TestingSet$transport_mode)))

print(model.svmRadial)
print(model.svmRadial.cv)

# Save the models
saveRDS(model.svmRadial, "models/model_svmRadial.rds")
saveRDS(model.svmRadial.cv, "models/model_svmRadial_cv.rds")
```
## Reduced Number of Classes
We found the best tune to be sigma = 0.8683492 with C = 1

## All Classes

```{r}
set.seed(100)

# Build Training Model
model.svmPoly <- train(transport_mode ~ ., 
               data = TrainingSet,
               method = "svmPoly",
               na.action = na.omit,
               preprocess = c("sclae","center"),
               trControl = trainControl(method = "none"),
               tuneGrid = data.frame(degree = 3, scale = 0.1, C = 4)
               )
               

# Build CV Model (long processing)
TrainingSet$transport_mode <- as.character(TrainingSet$transport_mode)
model.svmPoly.cv <- train(transport_mode ~ ., 
               data = TrainingSet,
               method = "svmPoly",
               na.action = na.omit,
               preprocess = c("sclae","center"),
               trControl = fitControl,
               tuneGrid = data.frame(degree = 3, scale = 0.1, C = 4) # Fit Model) 
               )
                
               
(model.svmPoly.cv$bestTune)

# Make Predictions
model.svmPoly.training <- predict(model.svmPoly, TrainingSet)
model.svmPoly.testing <- predict(model.svmPoly, TestingSet)

# Make Predictions from Cross Validation model
model.svmPoly.cv.training <- predict(model.svmPoly.cv, TrainingSet)
model.svmPoly.cv.testing <- predict(model.svmPoly.cv, TestingSet)

# Model Performance
(model.svmPoly.training.confusion <- confusionMatrix(model.svmPoly.training, as.factor(TrainingSet$transport_mode)))
(model.svmPoly.testing.confusion <- confusionMatrix(model.svmPoly.testing, as.factor(TestingSet$transport_mode)))
(model.svmPoly.cv.confusion <- confusionMatrix(model.svmPoly.cv.training, as.factor(TrainingSet$transport_mode)))
(model.svmPoly.cv.testing.confusion <- confusionMatrix(model.svmPoly.cv.testing, as.factor(TestingSet$transport_mode)))

print(model.svmPoly)
print(model.svmPoly.cv)

# Save the models
saveRDS(model.svmPoly, "models/model_svmPoly.rds")
saveRDS(model.svmPoly.cv, "models/model_svmPoly_cv.rds")
```
# CHAT GPT!!!
There could be several reasons why you might observe lower accuracy when using cross-validation compared to a single train-test split. Here are a few possibilities:

1. Smaller training set: Cross-validation involves dividing the data into multiple folds or subsets and using each fold as both a training set and a validation set. This means that the individual training sets used in each iteration of cross-validation might be smaller than the entire dataset used in a single train-test split. With a smaller training set, the model might have less data to learn from, leading to potentially lower accuracy.

2. Increased model generalization: Cross-validation helps assess the model's ability to generalize well to unseen data. By evaluating the model on multiple validation sets, it provides a more robust estimate of its performance. However, this also means that the model needs to perform consistently well across different subsets of data, which might be more challenging than performing well on a single train-test split. If the model struggles with generalization, the accuracy obtained during cross-validation might be lower.

3. Data variability: Cross-validation can reveal the variability of the model's performance across different subsets of data. If the data contains inherent variability or if there are outliers or inconsistencies in the data, the model's performance might vary across folds. This variability can result in lower accuracy when averaged across multiple folds during cross-validation compared to a single train-test split.

4. Hyperparameter tuning: Cross-validation is often used for hyperparameter tuning, where different combinations of hyperparameters are evaluated on the validation sets. If the hyperparameters are not tuned properly or if the search space for hyperparameters is limited, the model's performance might suffer, leading to lower accuracy during cross-validation.

It's important to note that while accuracy is a commonly used evaluation metric, it may not always be the most suitable metric for assessing model performance, especially in imbalanced datasets or when the cost of misclassifications differs across classes. It's advisable to consider other metrics like precision, recall, F1 score, or area under the ROC curve (AUC) depending on the specific problem and data characteristics.

If you suspect that there might be issues with the model or the data, it's worth investigating further, such as analyzing the model's learning curves, examining misclassified instances, or performing additional data preprocessing or feature engineering steps to potentially improve the model's performance.

# END CHATGPT

```{r}
# Model Overview

# Choose best model
```


```{r}
# Set seed for reprocudeability
set.seed(100)

# Run Model on full dataset
model.final <- predict(model.svmL.cv, working_dataset)
model.final.confusion <- confusionMatrix(model.final, as.factor(working_dataset$transport_mode))

# Save and Print final model confusion matrix
(model.final.table <- model.final.confusion[[2]])

working_dataset_result <- data.frame(working_dataset, model.final) 
working_dataset_result <- unique(working_dataset_result)

working_dataset_result <- left_join(working_dataset_result, working_dataset_copy, 
                                    by = c("angle", "diff_s", "steplength", "velocity", "step_mean", "diff_s_mean", "velocity_mean", "acceleration", "acceleration_mean", "sinuosity", "distance_tram", "distance_zug", "distance_gewaesser", "distance_bus", "distance_strasse")) |>
  rename(transport_mode = transport_mode.x)

working_dataset_result <- working_dataset_result[,-c(19,20)]

# Save working_dataset_result as a CSV file
write.csv(working_dataset_result, "data/working_dataset_result.csv", row.names = FALSE)


```
```{r}
# Show class distribution
final_classes <- ggplot(working_dataset_result) + 
  geom_bar(aes(x = model.final)) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  ylim(c(0,700)) +
  geom_hline(yintercept = 500, colour = "red", linetype = "dashed")

classes <- ggplot(working_dataset_result) + 
  geom_bar(aes(x = transport_mode)) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  ylim(c(0,700)) +
  geom_hline(yintercept = 500, colour = "red", linetype = "dashed")

grid.arrange(classes, final_classes, nrow = 1)
```


```{r}

working_dataset_result <- read.delim("data/working_dataset_result.csv", sep=",", header = T) 

working_dataset_result <- working_dataset_result %>%
    st_as_sf(coords = c("longitude", "latitude"), crs = 4326) 

map <- mapview(working_dataset_result, 
               zcol = "transport_mode", 
               layer.name = "Transport Mode",
               alpha.regions = 0.6,
               size = 3) +
  mapview(working_dataset_result, zcol = "model.final")

# Show the interactive map
map
```


```{r}
# Map with only wrong points

wrong_points <- working_dataset_result[!(working_dataset_result$transport_mode == working_dataset_result$model.final), ]

map <- mapview(wrong_points, 
               zcol = "transport_mode", 
               layer.name = "Transport Mode",
               alpha.regions = 0.6,
               size = 3) +
  mapview(working_dataset_result, zcol = "model.final", layer.name = "Model Classification")

# Show the interactive map
map
```



```{r Accuracy}

# Confusion Matrix
# Note: Only works if result and working dataset have identical transport modes as factors.
(conf_matrix <- confusionMatrix(working_dataset_result$transport_mode, working_dataset_result$model.final))

# Precision for each class
(precision <- conf_matrix$byClass[, "Precision"])

# Recall for each class
(recall <- conf_matrix$byClass[, "Recall"])

# F1-Score for each class
(f1_score <- conf_matrix$byClass[, "F1"])
```

Confusion Matrix: 

Precision: Precision measures the proportion of correctly predicted positive instances out of the total 
instances predicted as positive. 
It is useful when the focus is on minimizing false positives.

Recall (Sensitivity or True Positive Rate): Recall measures the proportion of
correctly predicted positive instances out of the total actual positive instances.
It is useful when the goal is to minimize false negatives.

F1-Score: The F1-score combines precision and recall into a single metric.
It provides a balance between precision and recall and is useful when both false positives and false negatives are important.


# Post Processing

```{r}
# Run a loop to identify outlier points in classification. If prevous and following x points are identical, 
# but the middle one is different it is changed


# Define the number of previous and following points to consider
# x: Number of points to be looked at surrounding current value
# threshold_percentage: number of points which have to be equal so the current value gets changed
# i: number of iterations

single_point_correction <- function(df, x, threshold_percentage, iterations) {
  
  # Track the number of points changed
  changed_count <- 0  
  
  for (iter in 1:iterations) {
    for (i in (x + 1):(nrow(df) - x)) {
      current_value <- df$model.final[i]
      
      # Find x-Previous & x-Following Values around point i
      previous_values <- df$model.final[(i - x):(i - 1)]
      following_values <- df$model.final[(i + 1):(i + x)]
      
      # Calculate the number of occurrences for each class in the surrounding points
      class_counts <- table(c(previous_values, following_values))
      
      # Find the class that occurs most frequently
      most_frequent_class <- names(class_counts)[which.max(class_counts)]
      
      # Check if the most frequent class exceeds the threshold count
      if (class_counts[most_frequent_class] > threshold_percentage * length(c(previous_values, following_values))) {
        df$model.final[i] <- most_frequent_class
        changed_count <- changed_count + 1
      }
    }
  }
  
  message("Number of times the condition is true and values are updated:", changed_count)
  return(df)
}
 



working_dataset_result <- single_point_correction(working_dataset_result, 5, 0.75, 1)

```

```{r}
# Confusion Matrix for new results
(conf_matrix_2 <- confusionMatrix(as.factor(working_dataset_result$transport_mode), as.factor(working_dataset_result$model.final)))

# Precision for each class
(precision_2 <- conf_matrix_2$byClass[, "Precision"])

# Recall for each class
(recall_2 <- conf_matrix_2$byClass[, "Recall"])

# F1-Score for each class
(f1_score_2 <- conf_matrix_2$byClass[, "F1"])
```

# Classes after Post-Processing

```{r}
# Show class distribution
final_classes <- ggplot(working_dataset_result) + 
  geom_bar(aes(x = model.final)) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  ylim(c(0,700)) +
  geom_hline(yintercept = 500, colour = "red", linetype = "dashed")

classes <- ggplot(working_dataset_result) + 
  geom_bar(aes(x = transport_mode)) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  ylim(c(0,700)) +
  geom_hline(yintercept = 500, colour = "red", linetype = "dashed")

grid.arrange(classes, final_classes, nrow = 1)
```

# 6. Evaluation


# Visualisation

# Literature

https://findingspress.org/article/14520-classifying-transport-mode-from-global-positioning-systems-and-accelerometer-data-a-machine-learning-approach

